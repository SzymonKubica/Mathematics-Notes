\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{float}
\usepackage{verbatim}
\usepackage{amsthm}
\usepackage[shortlabels]{enumitem}
\renewcommand{\baselinestretch}{1.5}

\newcommand\F{\mathcal{F}}
\newcommand\lb{\left\lbrace}
\newcommand\rb{\right\rbrace}
\newcommand\w{\omega}
\newcommand\Q{\mathbb{Q}}
\newcommand\R{\mathbb{R}}
\newcommand\N{\mathbb{N}}
\newcommand\Sf{\mathcal{S}}
\newcommand\Lf{\mathcal{L}}
\newcommand\Bf{\mathcal{B}}
\newcommand\xb{\mathbf{x}}
\newcommand\yb{\mathbf{y}}
\newcommand\bb{\mathbf{b}}
\newcommand\vb{\mathbf{v}}
\newcommand\im{\textup{Im}}
\newcommand\dee{\text{d}}
\newcommand\st{\text{ such that }}
\newcommand\sumOfSeries{\sum_{n = 0}^{\infty}}
\newcommand\aMatrix{A \inMatrix }
\newcommand\inMatrix{\in M_{n \times m} (\R)}
\newcommand\inMatrixSq{\in M_{n \times n} (\R)}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{note}{Note}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}

\parindent 0ex
\begin{document}
\title{Mathematics Year I, Linear Algebra Term 1, 2 \\ Most Important theorems, definitions and propositions. } 
\date{\today}
\author{Szymon Kubica} 
\maketitle

\section{Introduction to matrices and vectors}
\begin{definition}
    The standard basis vectors for $ \R^n $ are the vectors
    \begin{align}
        e_1 &= \begin{pmatrix}
            1 \\
            0 \\ 
            \vdots \\
            0
        \end{pmatrix},
        \hspace{0.5cm}
        e_2 = \begin{pmatrix}
            0 \\
            1 \\ 
            \vdots \\
            0
        \end{pmatrix},
        \hspace{0.5cm}
        ...
        \hspace{0.5cm}
        e_n = \begin{pmatrix}
            0 \\
            0 \\ 
            \vdots \\
            1
        \end{pmatrix},
    \end{align}
\end{definition}

\begin{definition}
    Let $ v_1, ... v_n $ be vectors in $ \R^n $. Any expression of the form
    \[ \lambda_1v_1 + \lambda_2 v_2 + ... + \lambda_n v_n\]
    is called linear combination of the vectors $ v_1, ... v_n $.
\end{definition}

\begin{definition}
    The set of all linear combinations of a collection of vectors $ v_1, ... v_n $ is called the span of the vectors $ v_1, ... v_n $.
    Notation: \[span\{  v_1, ... v_n  \}\].
\end{definition}

\begin{note}
    $ R^n $ is equal to the span of the standard basis vectors.
\end{note}

\begin{definition}
    The norm of $ v $ is the non negative real number defined by 
    \[ ||v|| = \sqrt{v \cdot v}\].
\end{definition}

\begin{definition}
    A vector $ v \in \R^n $ is called a unit vector if $ ||v|| = 1 $.
\end{definition}

\begin{definition}
    Let $ u $ and $ v $ be vectors in $ \R^n $. The distance between $ u $ and $ v $ is defined by
    \[ dist(u, v) := || u - v ||\].
\end{definition}

\begin{definition}
    The $ (i, j) $ entry of a matrix is the entry in row $ i $ and column $ j $.
    \[\begin{pmatrix}
        a_{11} & a_{12} & ... & a_{1m} \\
        \vdots &        & \ddots & \vdots \\
        a_{n1} & a_{n2}       & ... & a{nm}
    \end{pmatrix}\]
    Most often we use the condensed notation $ M = (a_{ij}) $.
\end{definition}

\begin{definition}
    The transpose of an $ n \times m $ matrix $ A = (a_{ij}) $ is the $ m \times n $ matrix whose $ (i, j) $ entry is $ a_{ji} $. We denote it as $ A^T $.
\end{definition}

The leading diagonal of a matrix is the $ (1, 1), (2,2) ... $ entries. So the transpose is obtained by doing a reflection in the leading diagonal.

\begin{definition}
    The identity matrix $ I_n = (a_{ij}) $ is the square matrix such that 
    
    $ a_{ij} = 0 \; \forall \, i \neq j \text{, and } a_{ii} = 1 \text{, where } 0 < i,j \leq n$.
\end{definition}

\begin{definition}
    Let $ A = (a_{ij}) $ be a $ n \times m $ matrix and $\mathbf{b}$ be the column vector of height $ n $ and whose $i$th entry is $ b_i $. 
    Then $(v_1, ..., v_n)$ is a solution to the system
    \[
    \begin{cases}
        a_{11}x_1 + a_{12}x_2 + ... + a_{1m}x_m &= b_1 \\
        a_{21}x_1 + a_{22}x_2 + ... + a_{2m}x_m &= b_2 \\
        \vdots                                  &= \vdots \\
        a_{n1}x_1 + a_{n2}x_2 + ... + a_{nm}x_m &= b_n 
    \end{cases}
    \]
    if and only if the vector $\mathbf{v} \in \R^n $ with entries $ v_i $ is a solution of the equation
    \[ Av = b\]
    The matrix $ A $ is called the coefficient matrix of the system above. The augmented matrix associated to the system is the matrix obtained by adding $b$ as an extra column to $ A $.
    It is denoted as $ (A|b) $.
\end{definition}

\section{Row operations} 
\begin{definition}
    A \textbf{row operation} is one of the following procedures we can apply to a matrix:
    \begin{enumerate}
        \item $ r_i(\lambda) :$ Multiply each entry in the $i$th row by a real number $ \lambda \neq 0 $.
        \item $ r_{ij} :$ Swap row $i$ and row $j$. 
        \item $ r_{ij}(\lambda) :$ Add $\lambda$ times row $i$ to row $j$. 
    \end{enumerate}
    \label{rowOps}
\end{definition}

\begin{proposition}
    Let $Ax = \mathbf{b} $ be a system of linear equations in matrix form. Let $ r $ be one of the row operations from Definition \ref{rowOps}, 
    and let $(A'| \mathbf{b}')$ be the result of applying $r$ to the augmented matrix $(A| \mathbf{b})$. Then the vector $\mathbf{v}$ is a solution of 
    $Ax = \mathbf{b} $ if and only if it is a solution of $A'x = \mathbf{b}' $.
\end{proposition}

\section{A systematical way of solving linear systems.}
\begin{definition}
    The left-most non-zero entry in a non-zero row is called the \textbf{leading entry} of that row.
\end{definition}

\begin{definition}
    A matrix is in \textbf{echelon form} if 
    \begin{enumerate}
        \item the leading entry in each non-zero row is $1$,
        \item the leading 1 in each non-zero row is to the right of the leading 1 in any row above it,
        \item the zero rows are below any non-zero rows. 
    \end{enumerate}
\end{definition}

\begin{definition}
    A matrix is in \textbf{row reduced echelon form (RRE)} if 
    \begin{enumerate}
        \item it is in echelon form,
        \item the leading entry in each non zero row is the only non-zero entry in its column.
    \end{enumerate}
\end{definition}
Solution algorithm

If we have a system of equations 
\[ Ax = b\]
and $ A $ is in RRE form, then we can easily read off the solutions (if any exist). There are four cases to consider.
\begin{itemize}
    \item \textbf{Case 1.} Every column of $A$ contains a leading 1, and there are no zero rows. In this cas the only possibility is that $ A = I_n $.
    Then the equations are simply
    \begin{equation}
        \begin{split}
            x_1 &= b_1 \\
            x_2 &= b_2 \\
            \vdots & \\
            x_n &= b_n \\
        \end{split}
    \end{equation}
    and they have a unique solution, i.e. the entries of $b$.
    \item \textbf{Case 2.} Every column of $A$ contains a leading 1, and there are some zero rows. Then $A$ must have more rows than column, and it must be a matrix of the from
    \begin{align}
        A &= \begin{pmatrix}
            I_n \\
            \mathbf{0}_{k \times n}
        \end{pmatrix}
    \end{align}
    Which looks like an identity matrix with a block of zero rows underneath. In this case the respective equations are 
    \begin{equation}
        \begin{split}
            x_1 &= b_1 \\
            \vdots & \\
            x_n &= b_n \\
        \end{split}
    \end{equation}
    And the last $k$ of them are
    \begin{equation}
        \begin{split}
            0 &= b_{n+1} \\
            \vdots & \\
            x_n &= b_{n + k}. \\
        \end{split}
    \end{equation}
    Now there are two possibilities:
    \begin{enumerate}
        \item If any of the last $k$ entries of $\mathbf{b}$ are non-zero then this system has no solutions, i.e. it is inconsistent.
        \item If the last $k$ entries of $\mathbf{b}$ are all zero then the system has a unique solution, given by setting $ x_i = b_i \;\forall \; i \in [1, n] $.
    \end{enumerate}
    \item \textbf{Case 3.} Some columns of $A$ do not contain a leading 1, but there are no zero rows. For example
    \begin{align}
        A &= \begin{pmatrix}
            1 & a_{12} & 0 \\
            0 & 0 & 1 \\
        \end{pmatrix}.
    \end{align}
    If the $i$th column of $A$ does not contain a leading 1, then the corresponding $x_i$ is called a \textbf{free variable}. 
    The remaining variables are called basic variables. This kind of system has infinitely many solutions, we call such system underdetermined.
\end{itemize}

See the full version of the notes for a step by step proof-algorithm how to turn any matrix into RRE form.

Now we have a systematic procedure for solving a system of simultaneous linear equations $Ax = \mathbf{b}$.
\begin{enumerate}
    \item Form the augmented matrix $(A|\mathbf{b})$.
    \item Apply the algorithm to put the augmented matrix into RRE form $(A'|\mathbf{b}')$.
    \item Read off the solutions to $A'x = \mathbf{b}'$.
\end{enumerate}
\begin{note}
    In fact we only need to put the left block $A'$ to be able to read off the solutions.
\end{note}

\begin{proposition}
    The number of solutions to a system $Ax = \mathbf{b}$ is always either 0, 1 or $\infty$. 
\end{proposition}

\section{Matrix Multiplication}
\begin{remark}
   The easiest way to remember how to multiply matrices is as follows 
   
   In order to multiply $AB$ where $A \in M_{n \times m} (\R) $ and $B \in M_{m \times p} (\R) $ we write
   $ r_1, r_2, ..., r_n \in \R^m $ for the rows of $A$ and $ c_1, c_2, ..., c_n \in \R^m $ for the columns of $B$. 
   Then the definitions states that the $(i, j)$ entry of $AB \in M_{n \times p} (\R) $ is the dot product
   \[ r_i^T \cdot c_j\]
   We can view this as in order to determine one column of $AB$ we take the respective column of $B$ and scan $A$ with it from top to bottom multiplying each row of $A$ 
   by the current column.
\end{remark}

\begin{definition}
    A matrix $ A \in M_{n \times m} (R) $ defines a function from $\R^m $ to $ \R^n $, which we denote by
    \[\begin{split}
        T_A : \R^m &\to \R^n \\
        \mathbf{v} &\mapsto A\mathbf{v}.
    \end{split}
    \]
\end{definition}
Provided that two matrices have correct dimensions, we can compose the functions defined by them.
\begin{lemma}
    $T_A \circ T_B = T_{AB} $, i.e for any $\mathbf{v} \in \R^p $ we have
    \[ A(B\mathbf{v}) = (AB) \mathbf{v}.\]
    Where $A \in M_{n \times m} (\R) $ and $B \in M_{m \times p} (\R) $ 
\end{lemma}

\subsection{Matrix multiplication properties.}
\begin{proposition}
    Let $ A, A' \in M_{m \times n} (\R) $, let $ B, B' \in M_{n \times p} (\R) $ let $C \in M_{p \times q} (\R)$ Then the following holds. 
    \begin{enumerate}
        \item $A(BC) = (AB)C $ (associativity).
        \item $A(B + B') = AB + AB' $
        
        and

        $(A + A')B = AB + A'B $ 

        (left and right distributivity of multiplication over addition.)
        \item $ \forall \; \lambda \in \R (\lambda A)B = \lambda(AB) = A(\lambda B) $.
    \end{enumerate}
\end{proposition}

The usual rules about multiplication by zero and one translate onto matrix multiplication with a certain degree of caution. 

\begin{lemma}
    Let $ A \in M_{n \times m} (\R) $.
    \begin{enumerate}
        \item $ \forall \; k \in \N \; \mathbf{0}_{k \times n} A = \mathbf{0}_{k \times m} \text{ and } A\mathbf{0}_{m \times k}  = \mathbf{0}_{n \times k}. $
        \item $ I_n A = A = A I_m $.
    \end{enumerate}
\end{lemma}
\begin{lemma}
    Let $ diag(d_1, ..., d_n) $ and $ diag(d_1', ..., d_n') $ be two diagonal matrices $ \in M_{n \times n} (\R) $. 
    
    Then their product is $ diag(d_1 d_1', ..., d_n d_n') $.
\end{lemma}

\section{Inverse of a matrix}
\begin{definition}
    Let $ \aMatrix $ An $ n \times n $ matrix  $ A^{-1} $ such that 
    \[ AA^{-1} = A^{-1}A = I_n\]
    is called an \textbf{inverse} of $A$.
    A matrix $A$ is called \textbf{invertible} if the inverse exists, \textbf{singular} if not.
\end{definition}

\begin{lemma}
    \begin{enumerate}
        \item If $A$ is invertible, then the inverse is unique.
        \item If $A$ is invertible and either $AB = I_n$ or $BA = I_n$ for some $ B \inMatrixSq $, then $ B = A^{-1}$.
    \end{enumerate}
\end{lemma}

\begin{lemma}
    Suppose $ A, B \inMatrixSq $ are invertible. Then $AB$ is invertible and 
    \[ (AB)^{-1} = B^{-1}A^{-1}.\]
\end{lemma}

\begin{lemma}
    Let $ A \inMatrixSq $.
    \begin{enumerate}
        \item If there exists $ \mathbf{v} \neq 0 \in \R^n$ such that $ A\mathbf{v} = \mathbf{0}$ then $ A $ is not invertible.
        \item If there exists a non zero matrix $ B \inMatrixSq $ such that $ AB = \mathbf{0}_{n \times n} $ or  $ BA = \mathbf{0}_{n \times n} $
        then $ A $ is not invertible.
    \end{enumerate}
\end{lemma}

\begin{corollary}
    If $ A \inMatrixSq $ has a column of zeros, then $ A $ is not invertible.
\end{corollary}

\begin{corollary}
    If $ A \inMatrixSq $ has a row of zeros, then $ A $ is not invertible.
\end{corollary}

\begin{lemma}
    If $ A \inMatrixSq $ and $ A' $ is obtained from $ A $ by a row operation, then $ A' $ is invertible if and only if $ A $ is.
\end{lemma}

\begin{lemma}
    If $ A \inMatrixSq $ and $ A' $ is the RRE form of $ A $ , then $ A' $ is invertible if and only if it has no zero rows.
\end{lemma}

\begin{lemma}
    A matrix $ A \inMatrixSq $  is invertible if and only if its RRE form is the identity matrix.
\end{lemma}

\begin{proposition}
    A matrix $ A \inMatrixSq $ is invertible if and only if there is no non-zero vector $ \mathbf{v} \in \R^n \st A\mathbf{v} = \mathbf{0} $.
\end{proposition}

A following algorithm can be used for computing the inverse of a matrix.
\begin{enumerate}
    \item Write the augmented matrix $ (A|I_n) $.
    \item Row reduce $ (A|I_n) $ and bring it into RRE form.
    \item If $A$ is invertible this process will transform the $A$ to its RRE form and $I_n$ to $A^{-1}$.
\end{enumerate}

\begin{remark}
    The algorithm described above gives us another method for finding solutions of linear systems of equations. Assuming we have such a system in matrix form $ Ax = b $,
    and $A$ is an invertible matrix, we can compute $A^{-1}$ and rewrite our system as $ x = A^{-1}b$. Note that it works if and only if $A$ is invertible
    and therefore the system has a unique solution.
\end{remark}

\section{Vector spaces}
\begin{definition}
    A \textbf{vector space} is the following data:
    \begin{itemize}
        \item A set $V$. We will refer to the elements of $V$ as vectors.
        \item A binary operation $ + \, : V \times V \to V $, which we call addition.
        \item A function $ \R \times V \to V $, which we call scalar multiplication. We usually omit the symbol of it.
    \end{itemize}
    We require that the following axioms hold:
    \begin{enumerate}
        \item The set $V$ with the binary operation $+$ forms an Abelian group. We denote the identity element of it as $\mathbf{0}_V$
        \item $ \forall \; \mathbf{v} \in V \text{ we have } 1\mathbf{v} = \mathbf{v} $.
        \item $ \forall \; \lambda, \mu \in \R \; \forall \; \mathbf{v} \in V \text{ we have } \lambda (\mu \mathbf{v}) = (\lambda\mu)\mathbf{v} $.
        \item $ \forall \; \lambda, \in \R \; \forall \; \mathbf{u}, \mathbf{v} \in V \text{ we have } \lambda (\mathbf{u} + \mathbf{v}) = \lambda\mathbf{u} + \lambda\mathbf{v}$.

        (distributivity of scalar multiplication over addition)
        \item $ \forall \; \lambda, \mu \in \R \; \forall \; \mathbf{v} \in V \text{ we have } (\lambda + \mu) \mathbf{v} = \lambda\mathbf{v}  + \mu\mathbf{v} $.

        (distributivity of scalar multiplication over scalar addition)
    \end{enumerate}
\end{definition}

\begin{lemma}
    Let $ V $ be a vector space and let $ \mathbf{x} \in V $.
    \begin{enumerate}
        \item For any positive integer $ n \in \R $, we have 
        \[ n\mathbf{x} = \mathbf{x} + \mathbf{x} + ... + \mathbf{x} \]
        where there are $ n $ terms on the right-hand-side.
        \item $0 \mathbf{x} = \mathbf(0)_V $.
        \item $ (-1)\mathbf{x} $ is the additive inverse of $ \mathbf{x} $.
    \end{enumerate}
\end{lemma}

\section{Subspaces}

\begin{definition}
    Let $ V $ be a vector space. A subset $ U \subset V $ is called a subspace of $ V $ if:
    \begin{enumerate}
        \item If $ \mathbf{x} + \mathbf{y} \in U $ then $ \mathbf{x} + \mathbf{y} \in U $ (Closure under vector addition). 
        \item $ \mathbf{0}_V \in U $.
        \item If $ \mathbf{x} \in U $ then $ \forall \; \lambda \in \R . \; \lambda \mathbf{x} \in U $.
    \end{enumerate}
\end{definition}

\section{Spanning sets}

\begin{definition}
    Let $ \mathcal{S} \subset V $ be any subset. A linear combination of elements in $ \mathcal{S} $ is a vector $ \mathbf{x} \in V $ which can be written as
    \[ \mathbf{x} = \lambda_1\mathbf{v_1} + \lambda_2\mathbf{v_2} + ... + \lambda_n\mathbf{v_n}\]
    where $ \mathbf{v_1}, \mathbf{v_2}, ..., \mathbf{v_n} $  are vectors in $ \mathcal{S} $, and $ \mathbf{\lambda_1}, \mathbf{\lambda_2}, ..., \mathbf{\lambda_n} $ are real numbers.
\end{definition}

\begin{lemma}
    A non-empty subset $ U $ of a vector space $ V $ is a subspace if and only if every linear combination of elements of $ U $ is again in $ U $.
\end{lemma}

\begin{definition}
    Let $ V $ be a vector space and let $ \mathcal{S} \subset V $ be a non-empty subset. 

    Then \textbf{span} of $ \mathcal{S} $ written 
    \[ \text{span } \mathcal{S} \subset V \]
    is the set of all linear combinations of elements of $ \mathcal{S} $. If $ \mathcal{S} = \emptyset$ then we define span $ \mathcal{S} $ to be $ \{\mathbf{0}_V\}$. 
\end{definition}

\begin{lemma}
    Let $ V $ be a vector space and let $ \mathcal{S} \subset V $ be any subset. Then $ \textup{span } \mathcal{S} $ is a subspace of $ V $.
\end{lemma}

\begin{definition}
    Let $ V $ be a vector space. A subset $ \mathcal{S} \subset V $ is called a \textbf{spanning set} if $ \text{span } \mathcal{S} = V $.
\end{definition}

\begin{lemma}
    Let $ V $ be a vector space, and let $ \Sf \subset V$ be any subset. Suppose $ \Sf \subset U $ for some subspace $ U \subset V $. Then $\textup{span } \Sf \subset U $. 
    So if $ \Sf $ is a spanning set for $ V $, then $ \Sf $ cannot be contained in any proper subspace $ U \subsetneq V$
\end{lemma}

\begin{definition}
    A vector space is called \textbf{finite-dimensional} if it has a finite spanning set. 
\end{definition}

\begin{definition}
    Let $ V $ be a finite-dimensional vector space. The dimension of $ V $, denoted as $ \text{dim } V $, is the smallest $ n \in \N $ such that $ V $ has a spanning set of size $ n $.
\end{definition}

\section{Linear independence}

\begin{definition}
    A subset $ \mathcal{L} $ of a vector space $ V $ is called linearly dependent if we can find \textbf{distinct} vectors $ \mathbf{v_1}, \mathbf{v_2}, ..., \mathbf{v_n} \in \mathcal{L} $
    and non-zero scalars $ \lambda_1 \neq 0,\; \lambda_2 \neq 0, \; ... \, ,\lambda_n \neq 0, $ such that  
    \[ \lambda_1\mathbf{v}_1 +\lambda_2\mathbf{v}_2 + ... + \lambda_n\mathbf{v}_n = \mathbf{0}_V. \]
\end{definition}
\begin{note}
    Any subset of a linearly independent set is also linearly-independent.
\end{note}

\begin{definition}
    A \textbf{basis} of a vector space is a linearly independent spanning set.
\end{definition}

\begin{proposition}
    Let $ \mathcal{B} = \{ \mathbf{v_1}, ..., \mathbf{v_n} \} \subset V $ be a finite basis of a vector space $ V $. Then every vector in $ V $ can be written as as linear combination of elements in $ \mathcal{B} $, in a unique way.
    Conversely, any finite subset $ \mathcal{B} $ with this property is a basis.
\end{proposition}

\section{Bases an dimension}

\begin{lemma}
    Let $ \mathcal{S} \subset V $ be a spanning set, and suppose that $\mathcal{S}$ is not linearly independent.
    Then there exists a vector $ \mathbf{x} \in \Sf \st \Sf' = \Sf  \; \backslash \{ \mathbf{x} \} $ is still a spanning set.
\end{lemma}

\begin{corollary}
    Any finite spanning set contains a basis. 
\end{corollary}

\begin{corollary}
    Any finite-dimensional vector space $ V $ has a basis.
\end{corollary}

\begin{proposition}[Steinitz exchange lemma - easy verion]
    Let $ \Sf \subset V $ be a spanning set, and let $ \mathbf{x} \in V $ be any non-zero vector. Then there exists a vector $ \mathbf{y} \in \Sf \st $ the set
    \[ \Sf' = (\Sf \backslash \{ \mathbf{y} \}) \cup \{ \mathbf{x} \} \]
    is still a spanning set.
\end{proposition}

\begin{proposition}[Steinitz exchange lemma - full verion]
    Let $ \Sf \subset V $ be a spanning set, and let $ \mathcal{L} \{ \mathbf{x}_1, ..., \mathbf{x}_n \}$ be a finite linearly independent subset of $ V $. 
    Then there exists a subset $ \mathcal{T} \{ \mathbf{y}_1, ..., \mathbf{y}_n \}\subset \Sf $, with the same size as $ \mathcal{L} $, such that  
    \[ \Sf' = (\Sf \backslash \mathcal{T}) \cup \mathcal{L} \]
    is still a spanning set.
\end{proposition}

\begin{corollary}
    Let $ V $ be a finite-dimensional vector space, let $ \Sf \subset V $ be a finite spanning set and let $ \mathcal{L} \subset V $ be a linearly independent subset. 
    Then $ \mathcal{L} $ is finite and $\# \mathcal{L} \leq \# \Sf $.
\end{corollary}

\begin{theorem}
    Let $ V $ be a finite-dimensional vector space with $ \textup{dim } V = n $. Then any basis of $ V $ is finite and has size $ n $.
\end{theorem}

\section{Dimensions of subspaces}

\begin{lemma}
    Suppose $ \mathcal{L} \subset V $ is a linearly independent subset. Let $ \mathbf{v} \in V $ be a vector which does not lie in $ \textup{span } \mathcal{L} $.
    Then $ \Lf \cup \{ \mathbf{v} \} $ is linearly independent.
\end{lemma} 

\begin{lemma}
    If $ V $ is not finite-dimensional, then for any $ n \in \N $ we can find linearly independent subset $ \Lf \subset V $ of size $ n $. 
\end{lemma}
 
\begin{lemma}
    Let $ V $ be a finite-dimensional vector space with $ \textup{dim } V = n $. Then any linearly independent subset $ \Lf \subset V $ of size $ n $ must be a basis.
\end{lemma}

\begin{lemma}
    If $ V $ is finite-dimensional then any linearly independent subset is contained in a basis.
\end{lemma}

\begin{proposition}
    Let $ V $ be a finite-dimensional vector space and let $ U \subset V $ be a subspace.
    \begin{enumerate}
        \item $ U $ is finite-dimensional.
        \item $ \textup{dim } U \leq \textup{dim } V$ .
        \item If $ \textup{dim } U = \textup{dim } V$ then $ U = V $.
    \end{enumerate}
\end{proposition}

\section{Linear maps}

\begin{definition}
    Let $ U $ and $ V $ be vector spaces. A function $ f : U \to V $ is called a \textbf{linear map} if
    \begin{itemize}
        \item $ f(\xb + \yb) = f(\xb) + f(\yb) $ for all $ \xb, \yb \in U $,
        \item $ f(\lambda\xb) = \lambda f(\xb) $ for all $ \xb \in U $ and all $ \lambda \in \R $.
    \end{itemize}
\end{definition}

\begin{lemma}
    If $ f : U \to V $ is linear then $ f(\mathbf{0}_U) =\mathbf{0}_V $. 
\end{lemma}

\begin{lemma}
    A function composition of linear maps is also a linear map.
\end{lemma}

\begin{definition}
    Let $ f : U \to V $ be a linear map. 
    \begin{itemize}
        \item The \textbf{image} of f, denoted as $ \im \, f $ is defined to be the subset 
        \[ \{ f(\xb)\, |\,  \xb \in U \} \subset V\]
        \item The \textbf{ker} of f, denoted as $ \ker \, f $ is defined to be the subset 
        \[ \{ \xb \in U\, |\,  f(\xb) = \mathbf{0}_V \} \subset U\]
    \end{itemize}
\end{definition}

\begin{lemma}
    Let $ f : U \to V $ be a linear map between vector spaces. Then $ \ker \, f $ is a subspace of $ U $ and $ \im \, f $ is a subspace of $ V $.
\end{lemma}

\begin{lemma}
    A linear map $ f : U \to V $ is injective if and only if $ \ker \, f = \{ \mathbf{0}_U \} $.
\end{lemma}

\begin{lemma}
    Let $ f : U \to V $ be a linear map, and fix $ \yb \in V $. Suppose $ \xb \in U $ is such that $ f(\xb) = \yb $. Then
    \[ f^{-1} (\yb) = \{\xb + \mathbf{v}\,  | \, \mathbf{v} \in \ker \, f\}\]
\end{lemma}

\section{Linear maps and bases}

\begin{proposition}
    Let $ f : \R^k \to \R^n $ be linear. Then $ f = T_A $ for some matrix $ A \in M_{n \times k} (\R) $
\end{proposition}

\begin{proposition}
    Let $ f : U \to V $ and $ g : U \to V $  be two linear maps, and let $ \Bf = \{ \bb_1, ..., \bb_k \} $ be a basis for $ U $. Suppose $ f(\bb_i) = g(\bb_i) $
    for each $ i = 1, ..., k. $ Then $ f = g $.
\end{proposition}

\begin{proposition}
    Let $ U $ and $ V $ be vector spaces. Let $ \Bf = \{ \bb_1, ..., \bb_k \} $ be a basis for $ U $, and let $ \{\vb_1, ..., \vb_k \} $ be any set of $ k $ vectors in $ V $.
    Then there is a unique linear map $ f : U \to V \st $ $ f (\bb_i) = \vb_i $ for each $ i $.
\end{proposition}

\section{Isomorphisms}
% TODO - page 45
\end{document}