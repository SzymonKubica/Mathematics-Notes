\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{float}
\usepackage{verbatim}
\usepackage{amsthm}
\usepackage[shortlabels]{enumitem}
\renewcommand{\baselinestretch}{1.5}

\newcommand\F{\mathcal{F}}
\newcommand\lb{\left\lbrace}
\newcommand\rb{\right\rbrace}
\newcommand\w{\omega}
\newcommand\Q{\mathbb{Q}}
\newcommand\R{\mathbb{R}}
\newcommand\N{\mathbb{N}}
\newcommand\Sf{\mathcal{S}}
\newcommand\Lf{\mathcal{L}}
\newcommand\Bf{\mathcal{B}}
\newcommand\Cf{\mathcal{C}}
\newcommand\xb{\mathbf{x}}
\newcommand\yb{\mathbf{y}}
\newcommand\bb{\mathbf{b}}
\newcommand\cb{\mathbf{c}}
\newcommand\eb{\mathbf{e}}
\newcommand\vb{\mathbf{v}}
\newcommand\im{\textup{Im}}
\newcommand\rank{\textup{rank}}
\newcommand\nullity{\textup{nullity}}
\newcommand\dee{\text{d}}
\newcommand\st{\text{ such that }}
\newcommand\sumOfSeries{\sum_{n = 0}^{\infty}}
\newcommand\aMatrix{A \inMatrix }
\newcommand\inMatrix{\in M_{n \times m} (\R)}
\newcommand\inMatrixSq{\in M_{n \times n} (\R)}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{note}{Note}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}

\parindent 0ex
\begin{document}
\title{Mathematics Year I, Linear Algebra Term 1, 2 \\ Most Important theorems, definitions and propositions. } 
\date{\today}
\author{Szymon Kubica} 
\maketitle

\section{Introduction to matrices and vectors}
\begin{definition}
    The standard basis vectors for $ \R^n $ are the vectors
    \begin{align}
        e_1 &= \begin{pmatrix}
            1 \\
            0 \\ 
            \vdots \\
            0
        \end{pmatrix},
        \hspace{0.5cm}
        e_2 = \begin{pmatrix}
            0 \\
            1 \\ 
            \vdots \\
            0
        \end{pmatrix},
        \hspace{0.5cm}
        ...
        \hspace{0.5cm}
        e_n = \begin{pmatrix}
            0 \\
            0 \\ 
            \vdots \\
            1
        \end{pmatrix},
    \end{align}
\end{definition}

\begin{definition}
    Let $ v_1, ... v_n $ be vectors in $ \R^n $. Any expression of the form
    \[ \lambda_1v_1 + \lambda_2 v_2 + ... + \lambda_n v_n\]
    is called linear combination of the vectors $ v_1, ... v_n $.
\end{definition}

\begin{definition}
    The set of all linear combinations of a collection of vectors $ v_1, ... v_n $ is called the span of the vectors $ v_1, ... v_n $.
    Notation: \[span\{  v_1, ... v_n  \}\].
\end{definition}

\begin{note}
    $ R^n $ is equal to the span of the standard basis vectors.
\end{note}

\begin{definition}
    The norm of $ v $ is the non negative real number defined by 
    \[ ||v|| = \sqrt{v \cdot v}\].
\end{definition}

\begin{definition}
    A vector $ v \in \R^n $ is called a unit vector if $ ||v|| = 1 $.
\end{definition}

\begin{definition}
    Let $ u $ and $ v $ be vectors in $ \R^n $. The distance between $ u $ and $ v $ is defined by
    \[ dist(u, v) := || u - v ||\].
\end{definition}

\begin{definition}
    The $ (i, j) $ entry of a matrix is the entry in row $ i $ and column $ j $.
    \[\begin{pmatrix}
        a_{11} & a_{12} & ... & a_{1m} \\
        \vdots &        & \ddots & \vdots \\
        a_{n1} & a_{n2}       & ... & a{nm}
    \end{pmatrix}\]
    Most often we use the condensed notation $ M = (a_{ij}) $.
\end{definition}

\begin{definition}
    The transpose of an $ n \times m $ matrix $ A = (a_{ij}) $ is the $ m \times n $ matrix whose $ (i, j) $ entry is $ a_{ji} $. We denote it as $ A^T $.
\end{definition}

The leading diagonal of a matrix is the $ (1, 1), (2,2) ... $ entries. So the transpose is obtained by doing a reflection in the leading diagonal.

\begin{definition}
    The identity matrix $ I_n = (a_{ij}) $ is the square matrix such that 
    
    $ a_{ij} = 0 \; \forall \, i \neq j \text{, and } a_{ii} = 1 \text{, where } 0 < i,j \leq n$.
\end{definition}

\begin{definition}
    Let $ A = (a_{ij}) $ be a $ n \times m $ matrix and $\mathbf{b}$ be the column vector of height $ n $ and whose $i$th entry is $ b_i $. 
    Then $(v_1, ..., v_n)$ is a solution to the system
    \[
    \begin{cases}
        a_{11}x_1 + a_{12}x_2 + ... + a_{1m}x_m &= b_1 \\
        a_{21}x_1 + a_{22}x_2 + ... + a_{2m}x_m &= b_2 \\
        \vdots                                  &= \vdots \\
        a_{n1}x_1 + a_{n2}x_2 + ... + a_{nm}x_m &= b_n 
    \end{cases}
    \]
    if and only if the vector $\mathbf{v} \in \R^n $ with entries $ v_i $ is a solution of the equation
    \[ Av = b\]
    The matrix $ A $ is called the coefficient matrix of the system above. The augmented matrix associated to the system is the matrix obtained by adding $b$ as an extra column to $ A $.
    It is denoted as $ (A|b) $.
\end{definition}

\section{Row operations} 
\begin{definition}
    A \textbf{row operation} is one of the following procedures we can apply to a matrix:
    \begin{enumerate}
        \item $ r_i(\lambda) :$ Multiply each entry in the $i$th row by a real number $ \lambda \neq 0 $.
        \item $ r_{ij} :$ Swap row $i$ and row $j$. 
        \item $ r_{ij}(\lambda) :$ Add $\lambda$ times row $i$ to row $j$. 
    \end{enumerate}
    \label{rowOps}
\end{definition}

\begin{proposition}
    Let $Ax = \mathbf{b} $ be a system of linear equations in matrix form. Let $ r $ be one of the row operations from Definition \ref{rowOps}, 
    and let $(A'| \mathbf{b}')$ be the result of applying $r$ to the augmented matrix $(A| \mathbf{b})$. Then the vector $\mathbf{v}$ is a solution of 
    $Ax = \mathbf{b} $ if and only if it is a solution of $A'x = \mathbf{b}' $.
\end{proposition}

\section{A systematical way of solving linear systems.}
\begin{definition}
    The left-most non-zero entry in a non-zero row is called the \textbf{leading entry} of that row.
\end{definition}

\begin{definition}
    A matrix is in \textbf{echelon form} if 
    \begin{enumerate}
        \item the leading entry in each non-zero row is $1$,
        \item the leading 1 in each non-zero row is to the right of the leading 1 in any row above it,
        \item the zero rows are below any non-zero rows. 
    \end{enumerate}
\end{definition}

\begin{definition}
    A matrix is in \textbf{row reduced echelon form (RRE)} if 
    \begin{enumerate}
        \item it is in echelon form,
        \item the leading entry in each non zero row is the only non-zero entry in its column.
    \end{enumerate}
\end{definition}
Solution algorithm

If we have a system of equations 
\[ Ax = b\]
and $ A $ is in RRE form, then we can easily read off the solutions (if any exist). There are four cases to consider.
\begin{itemize}
    \item \textbf{Case 1.} Every column of $A$ contains a leading 1, and there are no zero rows. In this cas the only possibility is that $ A = I_n $.
    Then the equations are simply
    \begin{equation}
        \begin{split}
            x_1 &= b_1 \\
            x_2 &= b_2 \\
            \vdots & \\
            x_n &= b_n \\
        \end{split}
    \end{equation}
    and they have a unique solution, i.e. the entries of $b$.
    \item \textbf{Case 2.} Every column of $A$ contains a leading 1, and there are some zero rows. Then $A$ must have more rows than column, and it must be a matrix of the from
    \begin{align}
        A &= \begin{pmatrix}
            I_n \\
            \mathbf{0}_{k \times n}
        \end{pmatrix}
    \end{align}
    Which looks like an identity matrix with a block of zero rows underneath. In this case the respective equations are 
    \begin{equation}
        \begin{split}
            x_1 &= b_1 \\
            \vdots & \\
            x_n &= b_n \\
        \end{split}
    \end{equation}
    And the last $k$ of them are
    \begin{equation}
        \begin{split}
            0 &= b_{n+1} \\
            \vdots & \\
            x_n &= b_{n + k}. \\
        \end{split}
    \end{equation}
    Now there are two possibilities:
    \begin{enumerate}
        \item If any of the last $k$ entries of $\mathbf{b}$ are non-zero then this system has no solutions, i.e. it is inconsistent.
        \item If the last $k$ entries of $\mathbf{b}$ are all zero then the system has a unique solution, given by setting $ x_i = b_i \;\forall \; i \in [1, n] $.
    \end{enumerate}
    \item \textbf{Case 3.} Some columns of $A$ do not contain a leading 1, but there are no zero rows. For example
    \begin{align}
        A &= \begin{pmatrix}
            1 & a_{12} & 0 \\
            0 & 0 & 1 \\
        \end{pmatrix}.
    \end{align}
    If the $i$th column of $A$ does not contain a leading 1, then the corresponding $x_i$ is called a \textbf{free variable}. 
    The remaining variables are called basic variables. This kind of system has infinitely many solutions, we call such system underdetermined.
\end{itemize}

See the full version of the notes for a step by step proof-algorithm how to turn any matrix into RRE form.

Now we have a systematic procedure for solving a system of simultaneous linear equations $Ax = \mathbf{b}$.
\begin{enumerate}
    \item Form the augmented matrix $(A|\mathbf{b})$.
    \item Apply the algorithm to put the augmented matrix into RRE form $(A'|\mathbf{b}')$.
    \item Read off the solutions to $A'x = \mathbf{b}'$.
\end{enumerate}
\begin{note}
    In fact we only need to put the left block $A'$ to be able to read off the solutions.
\end{note}

\begin{proposition}
    The number of solutions to a system $Ax = \mathbf{b}$ is always either 0, 1 or $\infty$. 
\end{proposition}

\section{Matrix Multiplication}
\begin{remark}
   The easiest way to remember how to multiply matrices is as follows 
   
   In order to multiply $AB$ where $A \in M_{n \times m} (\R) $ and $B \in M_{m \times p} (\R) $ we write
   $ r_1, r_2, ..., r_n \in \R^m $ for the rows of $A$ and $ c_1, c_2, ..., c_n \in \R^m $ for the columns of $B$. 
   Then the definitions states that the $(i, j)$ entry of $AB \in M_{n \times p} (\R) $ is the dot product
   \[ r_i^T \cdot c_j\]
   We can view this as in order to determine one column of $AB$ we take the respective column of $B$ and scan $A$ with it from top to bottom multiplying each row of $A$ 
   by the current column.
\end{remark}

\begin{definition}
    A matrix $ A \in M_{n \times m} (R) $ defines a function from $\R^m $ to $ \R^n $, which we denote by
    \[\begin{split}
        T_A : \R^m &\to \R^n \\
        \mathbf{v} &\mapsto A\mathbf{v}.
    \end{split}
    \]
\end{definition}
Provided that two matrices have correct dimensions, we can compose the functions defined by them.
\begin{lemma}
    $T_A \circ T_B = T_{AB} $, i.e for any $\mathbf{v} \in \R^p $ we have
    \[ A(B\mathbf{v}) = (AB) \mathbf{v}.\]
    Where $A \in M_{n \times m} (\R) $ and $B \in M_{m \times p} (\R) $ 
\end{lemma}

\subsection{Matrix multiplication properties.}
\begin{proposition}
    Let $ A, A' \in M_{m \times n} (\R) $, let $ B, B' \in M_{n \times p} (\R) $ let $C \in M_{p \times q} (\R)$ Then the following holds. 
    \begin{enumerate}
        \item $A(BC) = (AB)C $ (associativity).
        \item $A(B + B') = AB + AB' $
        
        and

        $(A + A')B = AB + A'B $ 

        (left and right distributivity of multiplication over addition.)
        \item $ \forall \; \lambda \in \R (\lambda A)B = \lambda(AB) = A(\lambda B) $.
    \end{enumerate}
\end{proposition}

The usual rules about multiplication by zero and one translate onto matrix multiplication with a certain degree of caution. 

\begin{lemma}
    Let $ A \in M_{n \times m} (\R) $.
    \begin{enumerate}
        \item $ \forall \; k \in \N \; \mathbf{0}_{k \times n} A = \mathbf{0}_{k \times m} \text{ and } A\mathbf{0}_{m \times k}  = \mathbf{0}_{n \times k}. $
        \item $ I_n A = A = A I_m $.
    \end{enumerate}
\end{lemma}
\begin{lemma}
    Let $ diag(d_1, ..., d_n) $ and $ diag(d_1', ..., d_n') $ be two diagonal matrices $ \in M_{n \times n} (\R) $. 
    
    Then their product is $ diag(d_1 d_1', ..., d_n d_n') $.
\end{lemma}

\section{Inverse of a matrix}
\begin{definition}
    Let $ \aMatrix $ An $ n \times n $ matrix  $ A^{-1} $ such that 
    \[ AA^{-1} = A^{-1}A = I_n\]
    is called an \textbf{inverse} of $A$.
    A matrix $A$ is called \textbf{invertible} if the inverse exists, \textbf{singular} if not.
\end{definition}

\begin{lemma}
    \begin{enumerate}
        \item If $A$ is invertible, then the inverse is unique.
        \item If $A$ is invertible and either $AB = I_n$ or $BA = I_n$ for some $ B \inMatrixSq $, then $ B = A^{-1}$.
    \end{enumerate}
\end{lemma}

\begin{lemma}
    Suppose $ A, B \inMatrixSq $ are invertible. Then $AB$ is invertible and 
    \[ (AB)^{-1} = B^{-1}A^{-1}.\]
\end{lemma}

\begin{lemma}
    Let $ A \inMatrixSq $.
    \begin{enumerate}
        \item If there exists $ \mathbf{v} \neq 0 \in \R^n$ such that $ A\mathbf{v} = \mathbf{0}$ then $ A $ is not invertible.
        \item If there exists a non zero matrix $ B \inMatrixSq $ such that $ AB = \mathbf{0}_{n \times n} $ or  $ BA = \mathbf{0}_{n \times n} $
        then $ A $ is not invertible.
    \end{enumerate}
\end{lemma}

\begin{corollary}
    If $ A \inMatrixSq $ has a column of zeros, then $ A $ is not invertible.
\end{corollary}

\begin{corollary}
    If $ A \inMatrixSq $ has a row of zeros, then $ A $ is not invertible.
\end{corollary}

\begin{lemma}
    If $ A \inMatrixSq $ and $ A' $ is obtained from $ A $ by a row operation, then $ A' $ is invertible if and only if $ A $ is.
\end{lemma}

\begin{lemma}
    If $ A \inMatrixSq $ and $ A' $ is the RRE form of $ A $ , then $ A' $ is invertible if and only if it has no zero rows.
\end{lemma}

\begin{lemma}
    A matrix $ A \inMatrixSq $  is invertible if and only if its RRE form is the identity matrix.
\end{lemma}

\begin{proposition}
    A matrix $ A \inMatrixSq $ is invertible if and only if there is no non-zero vector $ \mathbf{v} \in \R^n \st A\mathbf{v} = \mathbf{0} $.
\end{proposition}

A following algorithm can be used for computing the inverse of a matrix.
\begin{enumerate}
    \item Write the augmented matrix $ (A|I_n) $.
    \item Row reduce $ (A|I_n) $ and bring it into RRE form.
    \item If $A$ is invertible this process will transform the $A$ to its RRE form and $I_n$ to $A^{-1}$.
\end{enumerate}

\begin{remark}
    The algorithm described above gives us another method for finding solutions of linear systems of equations. Assuming we have such a system in matrix form $ Ax = b $,
    and $A$ is an invertible matrix, we can compute $A^{-1}$ and rewrite our system as $ x = A^{-1}b$. Note that it works if and only if $A$ is invertible
    and therefore the system has a unique solution.
\end{remark}

\section{Vector spaces}
\begin{definition}
    A \textbf{vector space} is the following data:
    \begin{itemize}
        \item A set $V$. We will refer to the elements of $V$ as vectors.
        \item A binary operation $ + \, : V \times V \to V $, which we call addition.
        \item A function $ \R \times V \to V $, which we call scalar multiplication. We usually omit the symbol of it.
    \end{itemize}
    We require that the following axioms hold:
    \begin{enumerate}
        \item The set $V$ with the binary operation $+$ forms an Abelian group. We denote the identity element of it as $\mathbf{0}_V$
        \item $ \forall \; \mathbf{v} \in V \text{ we have } 1\mathbf{v} = \mathbf{v} $.
        \item $ \forall \; \lambda, \mu \in \R \; \forall \; \mathbf{v} \in V \text{ we have } \lambda (\mu \mathbf{v}) = (\lambda\mu)\mathbf{v} $.
        \item $ \forall \; \lambda, \in \R \; \forall \; \mathbf{u}, \mathbf{v} \in V \text{ we have } \lambda (\mathbf{u} + \mathbf{v}) = \lambda\mathbf{u} + \lambda\mathbf{v}$.

        (distributivity of scalar multiplication over addition)
        \item $ \forall \; \lambda, \mu \in \R \; \forall \; \mathbf{v} \in V \text{ we have } (\lambda + \mu) \mathbf{v} = \lambda\mathbf{v}  + \mu\mathbf{v} $.

        (distributivity of scalar multiplication over scalar addition)
    \end{enumerate}
\end{definition}

\begin{lemma}
    Let $ V $ be a vector space and let $ \mathbf{x} \in V $.
    \begin{enumerate}
        \item For any positive integer $ n \in \R $, we have 
        \[ n\mathbf{x} = \mathbf{x} + \mathbf{x} + ... + \mathbf{x} \]
        where there are $ n $ terms on the right-hand-side.
        \item $0 \mathbf{x} = \mathbf(0)_V $.
        \item $ (-1)\mathbf{x} $ is the additive inverse of $ \mathbf{x} $.
    \end{enumerate}
\end{lemma}

\section{Subspaces}

\begin{definition}
    Let $ V $ be a vector space. A subset $ U \subset V $ is called a subspace of $ V $ if:
    \begin{enumerate}
        \item If $ \mathbf{x} + \mathbf{y} \in U $ then $ \mathbf{x} + \mathbf{y} \in U $ (Closure under vector addition). 
        \item $ \mathbf{0}_V \in U $.
        \item If $ \mathbf{x} \in U $ then $ \forall \; \lambda \in \R . \; \lambda \mathbf{x} \in U $.
    \end{enumerate}
\end{definition}

\section{Spanning sets}

\begin{definition}
    Let $ \mathcal{S} \subset V $ be any subset. A linear combination of elements in $ \mathcal{S} $ is a vector $ \mathbf{x} \in V $ which can be written as
    \[ \mathbf{x} = \lambda_1\mathbf{v_1} + \lambda_2\mathbf{v_2} + ... + \lambda_n\mathbf{v_n}\]
    where $ \mathbf{v_1}, \mathbf{v_2}, ..., \mathbf{v_n} $  are vectors in $ \mathcal{S} $, and $ \mathbf{\lambda_1}, \mathbf{\lambda_2}, ..., \mathbf{\lambda_n} $ are real numbers.
\end{definition}

\begin{lemma}
    A non-empty subset $ U $ of a vector space $ V $ is a subspace if and only if every linear combination of elements of $ U $ is again in $ U $.
\end{lemma}

\begin{definition}
    Let $ V $ be a vector space and let $ \mathcal{S} \subset V $ be a non-empty subset. 

    Then \textbf{span} of $ \mathcal{S} $ written 
    \[ \text{span } \mathcal{S} \subset V \]
    is the set of all linear combinations of elements of $ \mathcal{S} $. If $ \mathcal{S} = \emptyset$ then we define span $ \mathcal{S} $ to be $ \{\mathbf{0}_V\}$. 
\end{definition}

\begin{lemma}
    Let $ V $ be a vector space and let $ \mathcal{S} \subset V $ be any subset. Then $ \textup{span } \mathcal{S} $ is a subspace of $ V $.
\end{lemma}

\begin{definition}
    Let $ V $ be a vector space. A subset $ \mathcal{S} \subset V $ is called a \textbf{spanning set} if $ \text{span } \mathcal{S} = V $.
\end{definition}

\begin{lemma}
    Let $ V $ be a vector space, and let $ \Sf \subset V$ be any subset. Suppose $ \Sf \subset U $ for some subspace $ U \subset V $. Then $\textup{span } \Sf \subset U $. 
    So if $ \Sf $ is a spanning set for $ V $, then $ \Sf $ cannot be contained in any proper subspace $ U \subsetneq V$
\end{lemma}

\begin{definition}
    A vector space is called \textbf{finite-dimensional} if it has a finite spanning set. 
\end{definition}

\begin{definition}
    Let $ V $ be a finite-dimensional vector space. The dimension of $ V $, denoted as $ \text{dim } V $, is the smallest $ n \in \N $ such that $ V $ has a spanning set of size $ n $.
\end{definition}

\section{Linear independence}

\begin{definition}
    A subset $ \mathcal{L} $ of a vector space $ V $ is called linearly dependent if we can find \textbf{distinct} vectors $ \mathbf{v_1}, \mathbf{v_2}, ..., \mathbf{v_n} \in \mathcal{L} $
    and non-zero scalars $ \lambda_1 \neq 0,\; \lambda_2 \neq 0, \; ... \, ,\lambda_n \neq 0, $ such that  
    \[ \lambda_1\mathbf{v}_1 +\lambda_2\mathbf{v}_2 + ... + \lambda_n\mathbf{v}_n = \mathbf{0}_V. \]
\end{definition}
\begin{note}
    Any subset of a linearly independent set is also linearly-independent.
\end{note}

\begin{definition}
    A \textbf{basis} of a vector space is a linearly independent spanning set.
\end{definition}

\begin{proposition}
    Let $ \mathcal{B} = \{ \mathbf{v_1}, ..., \mathbf{v_n} \} \subset V $ be a finite basis of a vector space $ V $. Then every vector in $ V $ can be written as as linear combination of elements in $ \mathcal{B} $, in a unique way.
    Conversely, any finite subset $ \mathcal{B} $ with this property is a basis.
\end{proposition}

\section{Bases an dimension}

\begin{lemma}
    Let $ \mathcal{S} \subset V $ be a spanning set, and suppose that $\mathcal{S}$ is not linearly independent.
    Then there exists a vector $ \mathbf{x} \in \Sf \st \Sf' = \Sf  \; \backslash \{ \mathbf{x} \} $ is still a spanning set.
\end{lemma}

\begin{corollary}
    Any finite spanning set contains a basis. 
\end{corollary}

\begin{corollary}
    Any finite-dimensional vector space $ V $ has a basis.
\end{corollary}

\begin{proposition}[Steinitz exchange lemma - easy verion]
    Let $ \Sf \subset V $ be a spanning set, and let $ \mathbf{x} \in V $ be any non-zero vector. Then there exists a vector $ \mathbf{y} \in \Sf \st $ the set
    \[ \Sf' = (\Sf \backslash \{ \mathbf{y} \}) \cup \{ \mathbf{x} \} \]
    is still a spanning set.
\end{proposition}

\begin{proposition}[Steinitz exchange lemma - full verion]
    Let $ \Sf \subset V $ be a spanning set, and let $ \mathcal{L} \{ \mathbf{x}_1, ..., \mathbf{x}_n \}$ be a finite linearly independent subset of $ V $. 
    Then there exists a subset $ \mathcal{T} \{ \mathbf{y}_1, ..., \mathbf{y}_n \}\subset \Sf $, with the same size as $ \mathcal{L} $, such that  
    \[ \Sf' = (\Sf \backslash \mathcal{T}) \cup \mathcal{L} \]
    is still a spanning set.
\end{proposition}

\begin{corollary}
    Let $ V $ be a finite-dimensional vector space, let $ \Sf \subset V $ be a finite spanning set and let $ \mathcal{L} \subset V $ be a linearly independent subset. 
    Then $ \mathcal{L} $ is finite and $\# \mathcal{L} \leq \# \Sf $.
\end{corollary}

\begin{theorem}
    Let $ V $ be a finite-dimensional vector space with $ \textup{dim } V = n $. Then any basis of $ V $ is finite and has size $ n $.
\end{theorem}

\section{Dimensions of subspaces}

\begin{lemma}
    Suppose $ \mathcal{L} \subset V $ is a linearly independent subset. Let $ \mathbf{v} \in V $ be a vector which does not lie in $ \textup{span } \mathcal{L} $.
    Then $ \Lf \cup \{ \mathbf{v} \} $ is linearly independent.
\end{lemma} 

\begin{lemma}
    If $ V $ is not finite-dimensional, then for any $ n \in \N $ we can find linearly independent subset $ \Lf \subset V $ of size $ n $. 
\end{lemma}
 
\begin{lemma}
    Let $ V $ be a finite-dimensional vector space with $ \textup{dim } V = n $. Then any linearly independent subset $ \Lf \subset V $ of size $ n $ must be a basis.
\end{lemma}

\begin{lemma}
    If $ V $ is finite-dimensional then any linearly independent subset is contained in a basis.
\end{lemma}

\begin{proposition}
    Let $ V $ be a finite-dimensional vector space and let $ U \subset V $ be a subspace.
    \begin{enumerate}
        \item $ U $ is finite-dimensional.
        \item $ \textup{dim } U \leq \textup{dim } V$ .
        \item If $ \textup{dim } U = \textup{dim } V$ then $ U = V $.
    \end{enumerate}
\end{proposition}

\section{Linear maps}

\begin{definition}
    Let $ U $ and $ V $ be vector spaces. A function $ f : U \to V $ is called a \textbf{linear map} if
    \begin{itemize}
        \item $ f(\xb + \yb) = f(\xb) + f(\yb) $ for all $ \xb, \yb \in U $,
        \item $ f(\lambda\xb) = \lambda f(\xb) $ for all $ \xb \in U $ and all $ \lambda \in \R $.
    \end{itemize}
\end{definition}

\begin{lemma}
    If $ f : U \to V $ is linear then $ f(\mathbf{0}_U) =\mathbf{0}_V $. 
\end{lemma}

\begin{lemma}
    A function composition of linear maps is also a linear map.
\end{lemma}

\begin{definition}
    Let $ f : U \to V $ be a linear map. 
    \begin{itemize}
        \item The \textbf{image} of f, denoted as $ \im \, f $ is defined to be the subset 
        \[ \{ f(\xb)\, |\,  \xb \in U \} \subset V\]
        \item The \textbf{ker} of f, denoted as $ \ker \, f $ is defined to be the subset 
        \[ \{ \xb \in U\, |\,  f(\xb) = \mathbf{0}_V \} \subset U\]
    \end{itemize}
\end{definition}

\begin{lemma}
    Let $ f : U \to V $ be a linear map between vector spaces. Then $ \ker \, f $ is a subspace of $ U $ and $ \im \, f $ is a subspace of $ V $.
\end{lemma}

\begin{lemma}
    A linear map $ f : U \to V $ is injective if and only if $ \ker \, f = \{ \mathbf{0}_U \} $.
\end{lemma}

\begin{lemma}
    Let $ f : U \to V $ be a linear map, and fix $ \yb \in V $. Suppose $ \xb \in U $ is such that $ f(\xb) = \yb $. Then
    \[ f^{-1} (\yb) = \{\xb + \mathbf{v}\,  | \, \mathbf{v} \in \ker \, f\}\]
\end{lemma}

\section{Linear maps and bases}

\begin{proposition}
    Let $ f : \R^k \to \R^n $ be linear. Then $ f = T_A $ for some matrix $ A \in M_{n \times k} (\R) $
\end{proposition}

\begin{proposition}
    Let $ f : U \to V $ and $ g : U \to V $  be two linear maps, and let $ \Bf = \{ \bb_1, ..., \bb_k \} $ be a basis for $ U $. Suppose $ f(\bb_i) = g(\bb_i) $
    for each $ i = 1, ..., k. $ Then $ f = g $.
\end{proposition}

\begin{proposition}
    Let $ U $ and $ V $ be vector spaces. Let $ \Bf = \{ \bb_1, ..., \bb_k \} $ be a basis for $ U $, and let $ \{\vb_1, ..., \vb_k \} $ be any set of $ k $ vectors in $ V $.
    Then there is a unique linear map $ f : U \to V \st $ $ f (\bb_i) = \vb_i $ for each $ i $.
\end{proposition}

\section{Isomorphisms}

\begin{definition}
    A linear map $ f : U \to V $ between two vector spaces is called an \textbf{isomorphism} if $ f $ is bijective. If there exists an isomorphism from $ U $ to $ V $ 
    we say that $ U $ is isomorphic to $ V $ and write
    \[ U \cong V.\]
\end{definition}

\begin{proposition}
    Let $ V $ be a vector space with $ \textup{dim } V  = n$. Then $ V $ is isomorphic to $ \R^n $. 
\end{proposition}

\begin{lemma}
   Let $ f : U \to V $ be a linear map, and let $ \Bf = \{ \bb_1, ..., \bb_k \} $ be a basis for $ U $. Let $ \mathcal{C} = \{ f(\bb_1), ..., f(\bb_k) \} \subset V.$ 
   Then: 
   \begin{enumerate}
       \item $ \Cf $ is a spanning set if and only if $ f $ is surjective.
       \item $ \Cf $ is linearly independent if and only if $ f $ is injective.
       \item $ \Cf $ is a basis if and only if $ f $ is an isomorphism.
   \end{enumerate}
\end{lemma}

\begin{corollary}
    If $ U \cong V $ then $ \textup{dim } U = \textup{dim } V $.
\end{corollary}

\begin{corollary}
    Let $ f : U \to V $ be a linear map, and suppose $ \textup{dim } U = \textup{dim } V $. Then the following are equivalent:
    \begin{enumerate}
        \item $ f $ is injective.
        \item $ f $ is surjective. 
        \item $ f $ is an isomorphism. 
    \end{enumerate}
\end{corollary}

\begin{corollary}
    If $ f : \R^n \to V $ is an isomorphism, then the set
    \[ \Cf = \{ f(\eb_1), ..., f(\eb_n)\}\]
    is a basis for $ V $.
\end{corollary}

\section{The Rank-Nullity Theorem}
\begin{definition}
    Let $ U $ and $ V $ be vector spaces and let $ f : U \to V $ be a linear map. Then
    \begin{itemize}
        \item The \textbf{Rank} of $ f $ denoted as $\rank \, f$ is defined as $ \textup{dim } \im f $.
        \item The \textbf{Nullity} of $ f $ denoted as $\nullity \, f$ is defined as $ \dim \ker f $.
    \end{itemize}
\end{definition}

\begin{theorem}[Rank-Nullity theorem]
    Let $ U $ and $ V $ be vector spaces and let $ f : U \to V $ be a linear map. Then
    \[ \rank \, f + \nullity \, f = \dim \, U\].
\end{theorem}

\section{Linear maps and matrices}
Suppose $ U $ and $ V $ are vector spaces and $ f : U \to V $ is a linear map. We want to associate a matrix with $ f $ as we did before 
for a linear map between $ \R^n $ and $ \R^k $ In order to do it let 
\[ \Bf = \{\bb_1, ..., \bb_k\} \subset U \text{ and } \Cf = \{\cb_1, ..., \cb_n\} \subset V\]
be bases. We have seen in Proposition 12 that this gives us the following isomorphisms
\[ F_\Bf : \R^k \to U \text{ and } F_\Cf : \R^n \to V\]
Now by Lemma 20 the following composition of linear maps 
\[ F_\Cf^{-1} \circ f \circ F_\Bf \]
is also a linear map. Therefore it must be given by some  matrix $ A \in M_{n \times k} (\R)$ such that
\[ F_\Cf^{-1} \circ f \circ F_\Bf(\vb) = A \vb\]
This matrix $ A $ is called the \textbf{matrix representing $ f $ with respect to $ \Bf $ and $ \Cf $}, we denote it as:
\[ _\Cf[f]_\Bf \text{ or } [f]_{\Bf}^{\Cf}\]
To compute the matrix $_\Cf[f]_\Bf$ we can use the fact that the product $ A \eb_j $ is given by the $j$th column of $ A $.  
Therefore the $j$th column of $_\Cf[f]_\Bf$ is the vector
\[ F_\Cf^{-1} \circ f \circ F_\Bf(\eb_j) = F_\Cf^{-1} \circ f(\bb_j) \in \R^n.\]
The procedure of finding $_\Cf[f]_\Bf$ is as follows
\begin{itemize}
    \item For each $ j = 1, ..., k $, take the $j$th basis vector $ \bb_j \in \Bf $, and apply the map $ f $ to it to get a vector $f(\bb_j) \in V $
    \item Express each $f(\bb_j) $ as a linear combination of vectors in $\Cf$
    \[ f(\bb_j) = a_{1j}\cb_1 + a_{2j}\cb_2 + ... + a_{nj}\cb_n \]
    for some scalars $ a_{1j}, ..., a_{nj} \in \R $.

    Now $F_\Cf$ is a map that takes a vector in $ \R^n $ and returns a linear combination of the elements $\Cf$ (the basis of $V$) where scalar of the $i$th 
    basis vector of $V$ is the $i$th entry in the vector. Therefore 
    \begin{align}
        F_\Cf^{-1}( f(\bb_j)) &= \begin{pmatrix}
           a_{1j} \\
           a_{2j} \\
           \vdots \\
           a_{nj} 
        \end{pmatrix},
    \end{align}
    so $_\Cf[f]_\Bf$ is the matrix $(a{ij})$.
\end{itemize}

\begin{definition}
    Let $ V $ be a vector space, and let $\Bf = \{\bb_1, ..., \bb_k\}$ and $\Cf = \{\cb_1, ..., \cb_n\}$ be two bases for $V$. 
    The \textbf{change-of-basis matrix} from $\Bf$ to $\Cf$ is the matrix
    \[_\Cf[\text{id}_V]_\Bf\]
    That represents the identity map with respect to $ \Bf $ and $ \Cf $.
\end{definition}

\begin{lemma}
    Let $ V $ be a vector space, and let $\Bf = \{\bb_1, ..., \bb_k\}$ and $\Cf = \{\cb_1, ..., \cb_n\}$ be two bases for $V$,
    and let $ P = _\Cf[\text{id}_V]_\Bf $ be the change-of-basis matrix. Pick any $ \xb \in V $. If the coefficients of $\xb$ with respect to $\Bf$ are the vector
    $\vb \in \R $, then the coefficients of $\xb$ with respect to $\Cf$ are given by the vector 
    \[P\vb.\]
\end{lemma}

\begin{proposition}[Change-of-basis formula]
   Let $ f : U \to V $ be a linear map, Let $ \Bf $ and $ \Bf'$ be two bases for $ U $, and let $ _\Bf P_{\Bf'} $ be the change-of-basis matrix between them. 
   Also let $ \Cf $ and $ \Cf'$ be two bases for $ V $, and let $ _{\Cf'}P_{\Cf} $ be the change-of-basis matrix between them. Then the matrices representing $ f $ with respect
   to $\Bf $ and $ \Cf $ or with respect to $ \Bf' $ and $ \Cf' $ are related by 
   \[ _{\Cf'} [f]_{\Bf'} = \, _{\Cf'}P_\Cf \; _\Cf [f]_\Bf \; _\Bf P_{\Bf'}.\]
\end{proposition}

\section{Determinants}

\begin{definition}
    Let $ A $ be an $ n\times n $-matrix, and $ A_{ji} $ be the submatrix obtained by deleting the $i$-th row and $j$-th column of $ A $. $A_{ij}$ is called the minor of $ A $.
\end{definition}

\begin{definition}
    The determinant of an $ n\times n $-matrix $ A = (a_{ij}) $ is given by 
    \[ \det \, A = \sum_j (-1)^{i + j} \, a_{ij} \det A_{ij} = \sum_i (-1)^{i + j} \, a_{ij} \det A_{ij} \]
    The first sum is called expansion along the $i$-th row, the second is called expansion along the $j$-th column. 
\end{definition}

\begin{definition}
    Let $\sigma \in S_n $ be a permutation. An \textbf{inversion} $(i, j)$ of $\sigma$ is a pair of positive integers $i, j \st i < j \text{, but } \sigma(i) \geq \sigma(j) $.
    The \textbf{sign} of a permutation is defined to be 
    \[sgn(\sigma) := \begin{cases}
        1, \text{ if the number of inversions in } \sigma \text{ is even.} \\
        -1, \text{ if the number of inversions in } \sigma \text{ is odd.}
    \end{cases}\]
\end{definition}

\begin{definition}
    Let $ A = (a_{ij}) \inMatrixSq{} $. The \textbf{determinant} of $ A $ is the number:
    \[ \det(A) = \sum_{\sigma \in S_n} sgn(\sigma)a_{1\sigma(1)}a_{2\sigma(2)}...a_{n\sigma(n)}.\]
\end{definition}

\begin{proposition}
    Let $ A $ be a square matrix. Then
    \begin{enumerate}
        \item If 2 rows of $ A $ are swapped to produce a matrix $ B $, then $ \det \, B = - \det \, A $.
        \item If one row of $ A $ is multiplied by a scalar $ \lambda $ to produce a matrix $ B $, then $ \det \, B = \lambda \det \, A $.
        \item If a multiple of one row of $A$ is added to another row to produce a matrix $ B $, then $ \det \, B = \det \, A $.
    \end{enumerate}
\end{proposition}

\begin{proposition}
    Let $ A $ be a square matrix. Then
    \begin{enumerate}
        \item $ \det A = \det A^T $.
        \item $ \det AB = \det A \, \det B $. 
    \end{enumerate}
\end{proposition}

\begin{proposition}
    An $ n \times n $-matrix $ A $ is invertible if and only if $ \det \, A \neq 0 $.
\end{proposition}

\section{Eigenvalues and eigenvectors}

\begin{definition}
    Let $ A $ be a $ n \times n $- matrix, and let $\lambda \in \R $ be a number. We say that $ \lambda $ is an \textbf{eigenvalue} of $ A $ if there exists a non-zero vector
    $ \vb \in \R^n $ satisfying 
    \[ A \vb = \lambda \vb\]
    We call such a $ \vb $ an \textbf{eigenvector} of $ A $ (for the eigenvalue $ \lambda $).
\end{definition}

\begin{remark}
    Let $ A  \inMatrixSq $. Obviously, the non-zero vector $ \vb $ is an eigenvector of $ A $ with eigenvalue $ \lambda $ if and only if $ (A - \lambda I_n)\vb = \mathbf{0} $.
\end{remark}

\begin{proposition}
    Let $ A \inMatrixSq $. Then $ \lambda $ is an eigenvalue of $ A $ if and only if the matrix $ A - \lambda I_n $ is not invertible.
\end{proposition}

\begin{definition}
    Let $ A \inMatrixSq $. The determinant
    \[ p_A(\lambda) := \det(A - \lambda I_n)\]
    is called the characteristic polynomial.
\end{definition}

\section{Diagonalization}

\begin{definition}
    Let $ A $ and $ B $ be $ n \times n $-matrices. We call $ A $ similar to $ B $ if there exists an invertible matrix $ P \st A = P^{-1}BP$. 
\end{definition}

\begin{lemma}
    Let $ A $ and $ B $ be $ n \times n $-matrices. If $A$ is similar to $B$, then $B$ is similar to $A$.
\end{lemma}

\begin{proposition}
    Similar matrices have the same eigenvalues.
\end{proposition}

\begin{definition}
    A matrix $ A $ is called diagonalizable if it is similar to a diagonal matrix.
\end{definition}

\begin{lemma}
    Let $ A \inMatrixSq $. Suppose we can find $n$ eigenvectors $ \vb_1, ...,\vb_n $ of $ A $, with corresponding eigenvalues $\lambda_1, ..., \lambda_n$. Let
    \[ P := (\vb_1|\vb_2|...|\vb_n),\]
    i.e. the matrix whose columns are the vectors $ \vb_i $. If $ P $ is invertible, then $ A $ is diagonalizable and the corresponding diagonal matrix is given by 
    \[ P^{-1}AP = diag(\lambda_1, ..., \lambda_n).\]
\end{lemma}

\begin{definition}
    Let $ A \inMatrixSq $. The eigenspace $E_\lambda$ of $ A $ is the set
    \[ E_\lambda := \{ v \in \R^n | Av = \lambda v\}.\] 
    Note that this set includes all of the eigenvectors of $ A $ plus the zero vector.
\end{definition}

\begin{proposition}
    Any eigenspace $ E_\lambda $ of a matrix $ A \inMatrixSq $ is a subspace of $ \R^n $.
\end{proposition}

\begin{remark}
    The proposition above is not surprising as 
    \[ E_\lambda = \ker(T_{A - \lambda I_n}) = span\{v_1, ..., v_k\},\]
    where $ v_1, ..., v_k $ are the eigenvectors to the eigenvalue $\lambda$.

\end{remark}

\section{Gram-Schmidt Process and orthogonality}

\begin{definition}
    A set of vectors $ S = \{\vb_1,...,\vb_n\} \subset \R^n $ is called an orthogonal set if each pair of distinct vectors in the set is orthogonal:
    \[ v_i \cdot v_j = 0 \; \forall i, j, i\neq j.\]
    The set $ S $ is called orthonormal if it is orthonormal and all vectors in the set are unit vectors.
\end{definition}

\begin{proposition}
    Let $ \{\vb_1,...,\vb_n\} $ be an orthogonal set. Then $\{\vb_1,...,\vb_n\}$ is linearly independent.
\end{proposition}

\begin{definition}
    Let $ \{\vb_1,...,\vb_k\} $ be an orthogonal set of vectors in $ \R^n $ and $ U = span\{v_1,...,v_k\}.$ Let $ y \in \R^n $. The orthogonal projection of $ y $ onto $ U $
    is the vector given by
    \[ proj_U \; y := \frac{y \cdot v_1}{||v_1||^2}v_1 + \frac{y \cdot v_2}{||v_2||^2}v_2 +...+ \frac{y \cdot v_k}{||v_k||^2}v_k.\]  
\end{definition}

\begin{theorem}[Gram-Schmidt Process] 
    Let  $ \{\vb_1,...,\vb_k\} $ be a linearly independent set of vectors in $ \R^n $. Define inductively, $ i \leq k $:
    \begin{align}
        w_1 &:= v_1 \\
        w_2 &:= v_2 - proj_{span\{w_1\}} \; v_2 = v_2 - \frac{v_2 \cdot w_1}{||w_1||^2}w_1\\ 
        w_3 &:= v_3 - proj_{span\{w_1, w_2\}} \; v_3 = v_3 - \left(\frac{v_3 \cdot w_1}{||w_1||^2}w_1 + \frac{v_3 \cdot w_2}{||w_2||^2}w_2 \right)\\
        ... \\
        w_i &:= v_i - proj_{span\{w_1, w_2,..., w_{i -1}\}} \; v_i = v_i - \left(\frac{v_i \cdot w_1}{||w_1||^2}w_1 + \frac{v_i \cdot w_2}{||w_2||^2}w_2 + ... +\frac{v_i \cdot w_{i - 1}}{||w_{i - 1}||^2}w_{i - 1} \right)\\
    \end{align}
\end{theorem}
Then
\begin{enumerate}
    \item $ \{w_1,...,w_k\} $ is an orthogonal set.
    \item Let $ u_i = \frac{w_i}{||w_i||} $. Then $ \{u_1,...,u_k\} $ is an orthonormal set.
    \item $ span \{w_1,...,w_k\} = span \{u_1,...,u_k\} = span\{v_1,...,v_k\} $.
\end{enumerate}

\begin{definition}
    A matrix is called orthogonal if its columns as an orthogonal set. It is called orthonormal if it is orthogonal and the columns are unit vectors.
\end{definition}

\begin{note} 
    Since all columns of an orthogonal matrix are orthogonal, they are linearly independent and therefore an orthogonal matrix is always invertible.
\end{note}

\begin{proposition}
    A matrix $ A \inMatrixSq $ is orthogonal if and only if $ AA^T = I_n $.
\end{proposition}

\section{Real symmetric matrices and orthogonality}

\begin{definition}
    A matrix $ A \inMatrixSq $ is called symmetric if $ A = A^T $.
\end{definition}

\begin{proposition}
    Let $ A \inMatrixSq $, $ A $ symmetric. Then $ A $ has at least one real eigenvalue, and all its eigenvectors are real.
\end{proposition}

\begin{lemma}
    Let $ A \inMatrixSq $,$ A $ symmetric. Let $ \lambda_1, \lambda_2 $ be distinct eigenvalues of $ A $ with corresponding eigenvectors $ v_1 $ and $ v_2 $. Then $ v_1 $ and $ v_2 $ are orthogonal.
    \label{orthogonalEigenvectors}
\end{lemma}

\begin{theorem}
    Let $ A \inMatrixSq $,$ A $ symmetric. Then $ A $ is diagonalizable, i.e. there exists a diagonal matrix $ D $ and an invertible matrix $ P $ with
    $ A = PDP^{-1} $ such that the matrix $P$ of eigenvectors is orthogonal. We call such a matrix orthogonally diagonalizable.
    \label{orthogonallyDiag}
\end{theorem}

\begin{remark}
    \begin{enumerate}
        \item From theorem \ref{orthogonallyDiag} we can deduce that a symmetric matrix always has a basis of real eigenvectors. If all the eigenvalues are distinct, by 
        Lemma \ref{orthogonalEigenvectors}, we get immediately that the eigenvectors are pairwise orthogonal.
        \item Not every diagonalizable matrix is orthogonally diagonalizable. That is because the Gram-Schmidt process doesn't preserve eigenvectors in general. 
        Actually a matrix is orthogonally diagonalizable if and only if it is symmetric.
    \end{enumerate}
\end{remark}

\begin{theorem}[Spectral Theorem]
    A matrix $ A \inMatrixSq $ is orthogonally diagonalizable if and only is it is symmetric.
\end{theorem}
\end{document}